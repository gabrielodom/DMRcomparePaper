---
title: "DMR Method Comparisons"
author: "Gabriel J. Odom, PhD, ThD"
date: "July 5, 2018"
output:
  word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Load and Inspect the Results
## 1.1 Select Columns of Interest
For all four of the data sets, we can remove some of the superfluous results columns. We will retain the following columns: power, AuPR, precision, MCC, F1-score, effect size, random seed, quartiles of the number of CPGs retained at each design point and parameter point interaction, and the model parameters.
```{r trimCols, warning = FALSE}
resultsDMRcate_df <-
  dmrcateRes_df %>%
  select(power, AuPR, precision, mcc, F1,
         lambda, C,
         delta, seed, nCPG_med, nCPG_q3) %>%
  mutate(nCPG_med = as.numeric(nCPG_med)) %>%
  mutate(nCPG_q3 = as.numeric(nCPG_q3))

resultsPL_df <-
  probeLassoRes_df %>%
  select(power, AuPR, precision, mcc, F1,
         adjPval, mLassoRad, minDmrSep,
         delta, seed, nCPG_med, nCPG_q3) %>%
  mutate(nCPG_med = as.numeric(nCPG_med)) %>%
  mutate(nCPG_q3 = as.numeric(nCPG_q3))

resultsBump_df <-
  bumphunterRes_df %>%
  select(power, AuPR, precision, mcc, F1,
         cutoffQ, maxGap,
         delta, seed, nCPG_med, nCPG_q3) %>%
  mutate(nCPG_med = as.numeric(nCPG_med)) %>%
  mutate(nCPG_q3 = as.numeric(nCPG_q3))

resultsComb_df <-
  combpRes_df %>%
  select(power, AuPR, precision, mcc, F1,
         combSeed, combDist,
         delta, seed, nCPG_med, nCPG_q3) %>%
  mutate(nCPG_med = as.numeric(nCPG_med)) %>%
  mutate(nCPG_q3 = as.numeric(nCPG_q3))
```

## 1.2 Omit Missing Values
These data sets have missing values for many of the performance metrics with no treatment effect. We will remove all experiments with missing values.
```{r NAomit}
resultsDMRcate_df <- resultsDMRcate_df[complete.cases(resultsDMRcate_df), ]
resultsPL_df      <- resultsPL_df[complete.cases(resultsPL_df), ]
resultsBump_df    <- resultsBump_df[complete.cases(resultsBump_df), ]
resultsComb_df    <- resultsComb_df[complete.cases(resultsComb_df), ]
```


# 2 Spearman Correlations
Now that we have some clean data, we can inspect correlations between the different performance metrics and between model accuracy and parameter value.

## 2.1 Correlation Between Performance Metrics
We will now consider the Spearman correlation between AuPR and MCC and AuPR and the F1-score for each method.
```{r spearCorrs, echo = FALSE, warning = FALSE}
DMRc_cor <- cor(resultsDMRcate_df, method = "spearman")
pl_cor   <- cor(resultsPL_df, method = "spearman")
bump_cor <- cor(resultsBump_df, method = "spearman")
comb_cor <- cor(resultsComb_df, method = "spearman")

perfCorrs_df <- data.frame(
  DMRcate    = DMRc_cor[2, 4:5],
  ProbeLasso = pl_cor[2, 4:5],
  Bumphunter = bump_cor[2, 4:5],
  Comb_p     = comb_cor[2, 4:5]
)
rownames(perfCorrs_df) <- c("Cor(AuPR, MCC)", "Cor(AuPR, F1)")

library(knitr)
kable(perfCorrs_df)
```

Notice that for all methods, except for Bumphunter, the information conveyed by AuPR is nearly identical to that of the MCC or F1-score measures.

## 2.2 Correlation Between Parameters and Performance
Which parameters for each method have the strongest relationships with performance?
```{r spearParamCorrs, echo = FALSE}
SpearmanParam <- function(res_num, params_df, method_char){
  
  paramNames <- colnames(params_df)
  out_ls <- lapply(1:ncol(params_df), function(x){
    
    suppressWarnings(
      spear_ls <- cor.test(res_num, params_df[, x], method = "spearman")
    )
    
    inner_df <- data.frame(
      Method = method_char,
      Parameter = paramNames[x],
      Correlation = spear_ls$estimate,
      pValue = spear_ls$p.value,
      stringsAsFactors = FALSE
    )
    rownames(inner_df) <- NULL
    inner_df
    
  })
  
  bind_rows(out_ls)
}

dfs_ls <- list()
dfs_ls[[1]] <- SpearmanParam(resultsDMRcate_df$AuPR,
                              resultsDMRcate_df[, 6:7],
                              "DMRcate")
dfs_ls[[2]] <- SpearmanParam(resultsPL_df$AuPR,
                            resultsPL_df[, 6:8],
                            "ProbeLasso")
dfs_ls[[3]] <- SpearmanParam(resultsBump_df$AuPR,
                              resultsBump_df[, 6:7],
                              "Bumphunter")
dfs_ls[[4]] <- SpearmanParam(resultsComb_df$AuPR,
                              resultsComb_df[, 6:7],
                              "Comb-p")
spearmanCorrs_df <- bind_rows(dfs_ls)
# write_csv(spearmanCorrs_df, path = "../resultsData/spearmanCorrs.csv")
kable(spearmanCorrs_df)
```

###  More Detail on Bumphunter
We may also care to inspect the Bumphunter results in more detail: recall that the MCC and F1-score measures were not as highly correlated with AuPR for this method. We first tabulate Bumphunter's parameter effects on MCC.
```{r bumphunterCorrs1, echo=FALSE}
kable(SpearmanParam(resultsBump_df$mcc,
              resultsBump_df[, 6:7],
              "Bumphunter"))
```

Now we inspect Bumphunter's parameter effects on the F1-score.
```{r bumphunterCorrs2, echo=FALSE}
kable(SpearmanParam(resultsBump_df$F1,
              resultsBump_df[, 6:7],
              "Bumphunter"))
```


# 3 Best Parameter Settings
Now we will inspect which parameter settings yield the best AuPR values (averaged over each replicate) for each method. We will first trim each results data frame down to the AuPR, effect size, and parameters. We will also remove the design points with no effect.
```{r trimCols2}
resDMRc2_df <- 
  resultsDMRcate_df %>% 
  select(AuPR, delta, lambda, C) %>% 
  filter(delta > 0)

resPL2_df <- 
  resultsPL_df %>% 
  select(AuPR, delta, adjPval, mLassoRad, minDmrSep) %>% 
  filter(delta > 0)

resBump2_df <- 
  resultsBump_df %>% 
  select(AuPR, delta, cutoffQ, maxGap) %>% 
  filter(delta > 0)

resComb2_df <- 
  resultsComb_df %>% 
  select(AuPR, delta, combSeed, combDist) %>% 
  filter(delta > 0)
```

## 3.1 DMRcate
Now we can find the best DMRcate parameter settings for each value of $\delta$.
```{r bestDMRcateParams}
resDMRc2_df %>%
  group_by(delta, lambda, C) %>%
  summarise(AuPR_ave = mean(AuPR)) %>%
  group_by(delta) %>% 
  filter(AuPR_ave == max(AuPR_ave)) %>% 
  kable()
```

This table shows that smaller values of `lambda` paired with larger values of `C` often yield the best results. According to the `dmrcate` help documentation, the ratio of `lambda` and `C` yield the $\sigma$ parameter, and we believe that smaller values of this ratio yield more accurate results.


## 3.2 ProbeLasso
Now we can find the best ProbeLasso parameter settings for each value of $\delta$.
```{r bestProbeLassoParams}
resPL2_df %>%
  group_by(delta, adjPval, mLassoRad) %>%
  summarise(AuPR_ave = mean(AuPR)) %>%
  group_by(delta) %>% 
  filter(AuPR_ave == max(AuPR_ave)) %>% 
  kable()
```

As expected, larger values of the `meanLassoRadius` yield better AuPRs. What we did not expect was that the larger `adjPval` values also increased the AuPR. We have seen that smaller values of `adjPval` increased the effect of the `meanLassoRadius` parameter. Also, we removed the `minDmrSep` parameter from this table because it had no effect and created many ties (increasing the number of rows in this table without adding new information). We tried `minDmrSep = 200, 250, 500, 750, 1000`, and all five values yielded equal (to the thousandth place) AuPR values.

## 3.3 Bumphunter
Now we can find the best Bumphunter parameter settings for each value of $\delta$.
```{r bestBumphunterParams}
resBump2_df %>%
  group_by(delta, cutoffQ, maxGap) %>%
  summarise(AuPR_ave = mean(AuPR)) %>%
  group_by(delta) %>% 
  filter(AuPR_ave == max(AuPR_ave)) %>% 
  kable()
```

We saw in our preliminary results that `cutoffQ = 0.99` yielded poorer results, on average, so these results pertaining to the `cutoffQ` parameter do not surprise us. Further, we did not find any relationship between AuPR and the `maxGap` parameter in our preliminary work. If anything, we might take away that the smaller values of `maxGap` seem to fare better, and that `maxGap` should increase with effect size (data sets with stronger signal may need an increase in `maxGap` for best DMR-detection results).


## 3.4 Comb-p
Now we can find the best Comb-p parameter settings for each value of $\delta$.
```{r bestCombpParams}
resComb2_df %>%
  group_by(delta, combSeed, combDist) %>%
  summarise(AuPR_ave = mean(AuPR)) %>%
  group_by(delta) %>% 
  filter(AuPR_ave == max(AuPR_ave)) %>% 
  kable()
```

Once again, these results are in line with our preliminary expectations: the `combDist` parameter drives the AuPR results much more than the `combSeed` parameter. It seems that larger values of `combDist` are better, although the size may need to taper off when Comb-p is applied to data sets with strong signal. In our previous work, we found no effect on AuPR from the `combSeed` parameter, but we can keep it larger to be safe.
